{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 200000\n",
      "valid set size: 20001\n",
      "test set size: 39323\n",
      "{'english': '“We did find other tools that capture and analyze both dynamic and static queries, but they were limited to single queries,” Matthews says.', 'chinese': '“我们未发现其他工具能够同时采集和分析动态和静态查询，它们仅限于单种查询，” Matthews 说。'}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    "from data import TRANS\n",
    "\n",
    "max_dataset_size = 220000\n",
    "train_dataset_size = 200000\n",
    "valid_dataset_size = 20001\n",
    "\n",
    "data = TRANS('../../data/translation2019zh/translation2019zh_train.json')\n",
    "train_data, valid_data = random_split(data, [train_dataset_size, valid_dataset_size])\n",
    "test_data = TRANS('../../data/translation2019zh/translation2019zh_valid.json')    \n",
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁“', '我们', '未', '发现', '其他', '工具', '能够', '同时', '采集', '和分析', '动态', '和', '静', '态', '查询', ',', '它们', '仅限于', '单', '种', '查询', ',', '”', '▁Ma', 't', 'th', 'ew', 's', '▁', '说', '。', '</s>']\n",
      "['▁“', 'We', '▁did', '▁find', '▁other', '▁tools', '▁that', '▁capture', '▁and', '▁analyze', '▁both', '▁dynamic', '▁and', '▁static', '▁queries', ',', '▁but', '▁they', '▁were', '▁limited', '▁to', '▁single', '▁queries', ',', '”', '▁Matthew', 's', '▁says', '.', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "zh_sentence = train_data[0][\"chinese\"]\n",
    "en_sentence = train_data[0][\"english\"]\n",
    "\n",
    "inputs = tokenizer(zh_sentence)\n",
    "\n",
    "# # 注意添加上下文管理器\n",
    "# wrong_targets = tokenizer(en_sentence)\n",
    "# print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(en_sentence)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 41]), 'attention_mask': torch.Size([4, 41])}\n",
      "batch_y shape: torch.Size([4, 34])\n",
      "{'input_ids': tensor([[  196,   230,   997,  2075,   628,  1904,   577,  1120, 29517, 10116,\n",
      "          9241,    16, 13271, 20797, 12586,     2,   896, 14273,  3681,  4657,\n",
      "         12586,     2,   215,  4403,    59,   496, 10676,    22,     7,   300,\n",
      "             9,     0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000],\n",
      "        [    7,  5508,    11, 21332, 63068,  3244,   300,    35,  2272,    65,\n",
      "         21094,  1003,  2476,     2,    69, 23809,  1444,   149, 63275,  9602,\n",
      "            11, 52918,     2,  1444,  3848,    11, 15025, 17020,  8648, 51373,\n",
      "             2, 47969,    11,  7356, 28239,  1345,    11, 21938,  8906,     9,\n",
      "             0],\n",
      "        [ 3954,     2,  2130,    65, 44997,    11,  8283,  5518,  3215,   408,\n",
      "           486,     2,   330,  2130,  2433,  4925,     2,  1027, 16285, 17721,\n",
      "             9,     0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000],\n",
      "        [    7,    65,  3014,    31,  5886,  1381,   627,  4875,     2,  5774,\n",
      "         16676,  4704,  4225,  1896,     2, 24337,  8496,  5774,  1154,  2507,\n",
      "          8387,     9,     0, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[  196, 10562,   428,  1298,    85,  3224,    19, 16165,     6, 48660,\n",
      "           443, 11012,     6, 41365, 33680,     2,   183,   135,    84,  1690,\n",
      "             8,  2983, 33680,     2,   215, 44901,    22,  5089,     5,     0,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [60247,   466,     6,  5649,    14,     3,   235,     2,    30,    12,\n",
      "          2345,     8,     3, 43426, 49547,     2, 55757,     3, 24061,    88,\n",
      "            32,  2356,     2,    73,    32, 34670,   164,    12, 17508, 30351,\n",
      "         26181,    30,     5,     0],\n",
      "        [ 2657,  1303,     2,     3,  2047,  7217,     4,  1945,    30,    54,\n",
      "             3,   468,    33,    19,     4,  3473, 18223,  1457,     2,   183,\n",
      "          1945,    21,    22,  1858,    10,    56,   377,    88,    32, 32490,\n",
      "         48880,    49,     5,     0],\n",
      "        [56250,    22,  4867,    27,  9054, 18407, 33108,    10, 13846,     3,\n",
      "          8751,  5835,     4,     7, 45717,    22,  5762,   152,     6,   152,\n",
      "          6297,     2,     6,     3,     7, 31264, 58674,  2384,    42,  6887,\n",
      "          6315,     5,     0,  -100]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "inputs = [train_data[s_idx][\"chinese\"] for s_idx in range(4)]\n",
    "targets = [train_data[s_idx][\"english\"] for s_idx in range(4)]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    inputs, \n",
    "    padding=True, \n",
    "    max_length=max_input_length, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        padding=True, \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "for idx, end_idx in enumerate(end_token_index):\n",
    "    labels[idx][end_idx+1:] = -100 # 是否可以认为，pad设置为-100是为了让softmax之后的权重置为0？\n",
    "\n",
    "print('batch_X shape:', {k: v.shape for k, v in model_inputs.items()})\n",
    "print('batch_y shape:', labels.shape)\n",
    "print(model_inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['chinese'])\n",
    "        batch_targets.append(sample['english'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets,\n",
    "            padding=True,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"]\n",
    "        batch_data[\"decoder_input_ids\"] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx + 1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=32, shuffle=False, collate_fn=collote_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([32, 46]), 'attention_mask': torch.Size([32, 46]), 'decoder_input_ids': torch.Size([32, 48]), 'labels': torch.Size([32, 48])}\n",
      "{'input_ids': tensor([[    7,  5700,  3321,  ..., 65000, 65000, 65000],\n",
      "        [    7,  2620,   207,  ..., 65000, 65000, 65000],\n",
      "        [19979, 22446,   142,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [ 6302,    51,  1977,  ..., 65000, 65000, 65000],\n",
      "        [    7, 39443,  9913,  ..., 65000, 65000, 65000],\n",
      "        [ 1243,  4510,  6029,  ..., 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_input_ids': tensor([[65000,    24,  5090,  ..., 65000, 65000, 65000],\n",
      "        [65000, 16591,  2415,  ..., 65000, 65000, 65000],\n",
      "        [65000,  1738,    37,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [65000,    78,  1117,  ..., 65000, 65000, 65000],\n",
      "        [65000,    24, 47845,  ..., 65000, 65000, 65000],\n",
      "        [65000,    82,   307,  ..., 65000, 65000, 65000]]), 'labels': tensor([[   24,  5090,   698,  ...,  -100,  -100,  -100],\n",
      "        [16591,  2415, 55671,  ...,  -100,  -100,  -100],\n",
      "        [ 1738,    37,    44,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [   78,  1117, 48481,  ...,  -100,  -100,  -100],\n",
      "        [   24, 47845, 15808,  ...,  -100,  -100,  -100],\n",
      "        [   82,   307,    32,  ...,  -100,  -100,  -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练代码 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Helsinki-NLP/opus-mt-zh-en were not used when initializing MarianForMT: ['model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.3.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.3.fc1.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.0.fc2.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.1.fc1.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.shared.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.5.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.4.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.5.fc1.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.4.fc2.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.0.fc1.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing MarianForMT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarianForMT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarianForMT were not initialized from the model checkpoint at Helsinki-NLP/opus-mt-zh-en and are newly initialized: ['model.model.decoder.layers.3.self_attn.v_proj.bias', 'model.model.encoder.layers.3.self_attn.q_proj.bias', 'model.model.encoder.layers.3.self_attn.k_proj.bias', 'model.model.encoder.layers.3.fc1.bias', 'model.model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.model.decoder.layers.5.self_attn.out_proj.bias', 'model.model.decoder.layers.5.fc2.bias', 'model.model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.model.decoder.layers.1.self_attn.k_proj.weight', 'model.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.model.decoder.layers.4.fc1.bias', 'model.model.encoder.layers.2.fc2.bias', 'model.model.encoder.layers.0.fc2.weight', 'model.model.encoder.layers.0.self_attn.out_proj.bias', 'model.model.encoder.layers.4.self_attn.out_proj.weight', 'model.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.model.encoder.layers.5.self_attn.k_proj.bias', 'model.model.encoder.layers.4.fc1.bias', 'model.model.decoder.layers.1.self_attn.v_proj.bias', 'model.model.encoder.layers.2.self_attn.v_proj.bias', 'model.model.decoder.layers.5.self_attn.k_proj.weight', 'model.model.encoder.layers.5.self_attn.v_proj.weight', 'model.model.encoder.layers.2.fc1.weight', 'model.model.decoder.layers.1.self_attn.k_proj.bias', 'model.model.decoder.layers.1.self_attn.v_proj.weight', 'model.model.decoder.layers.3.fc1.weight', 'model.model.decoder.layers.0.fc2.bias', 'model.model.decoder.layers.1.self_attn.q_proj.weight', 'model.model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.model.decoder.layers.0.final_layer_norm.bias', 'model.model.encoder.layers.2.self_attn.out_proj.weight', 'model.model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.model.decoder.layers.3.fc2.weight', 'model.model.encoder.layers.5.fc1.weight', 'model.model.decoder.layers.5.self_attn.q_proj.bias', 'model.model.decoder.layers.4.self_attn.out_proj.bias', 'model.model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.model.decoder.layers.3.final_layer_norm.weight', 'model.model.decoder.layers.1.self_attn.q_proj.bias', 'model.model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.model.encoder.layers.2.self_attn.k_proj.weight', 'model.model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.model.encoder.layers.2.final_layer_norm.bias', 'model.model.decoder.layers.4.self_attn_layer_norm.bias', 'model.model.decoder.layers.2.fc1.weight', 'model.model.decoder.layers.2.fc1.bias', 'model.model.encoder.layers.5.fc1.bias', 'model.model.encoder.layers.2.fc2.weight', 'model.model.encoder.layers.3.self_attn.v_proj.bias', 'model.model.encoder.layers.3.fc2.bias', 'model.model.encoder.layers.4.self_attn_layer_norm.weight', 'model.model.encoder.layers.1.self_attn.k_proj.weight', 'model.model.encoder.layers.3.self_attn.k_proj.weight', 'model.model.decoder.layers.0.self_attn_layer_norm.bias', 'model.model.encoder.layers.5.self_attn_layer_norm.weight', 'model.model.decoder.layers.1.fc1.weight', 'model.model.decoder.layers.3.fc2.bias', 'model.model.decoder.layers.4.final_layer_norm.weight', 'model.model.encoder.layers.1.self_attn.q_proj.bias', 'model.model.decoder.layers.4.self_attn.v_proj.bias', 'model.model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.model.decoder.layers.3.self_attn.k_proj.bias', 'model.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.model.encoder.layers.4.self_attn.k_proj.weight', 'model.model.encoder.layers.2.self_attn.v_proj.weight', 'model.model.encoder.layers.1.final_layer_norm.weight', 'model.model.decoder.layers.3.self_attn.out_proj.bias', 'model.model.decoder.layers.5.self_attn_layer_norm.bias', 'model.model.decoder.layers.4.self_attn.q_proj.bias', 'model.model.decoder.layers.2.self_attn.q_proj.bias', 'model.model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.model.encoder.layers.1.self_attn.k_proj.bias', 'model.model.encoder.layers.5.fc2.weight', 'model.model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.model.decoder.layers.3.self_attn.q_proj.weight', 'model.model.decoder.layers.0.self_attn.out_proj.bias', 'model.model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.model.decoder.layers.5.fc1.weight', 'model.model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.model.decoder.layers.4.fc1.weight', 'model.model.decoder.layers.5.final_layer_norm.weight', 'model.model.decoder.layers.1.final_layer_norm.bias', 'model.model.decoder.layers.1.self_attn_layer_norm.weight', 'model.model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.model.decoder.layers.1.self_attn.out_proj.weight', 'model.model.decoder.layers.0.fc2.weight', 'model.model.encoder.layers.2.final_layer_norm.weight', 'model.model.decoder.layers.2.self_attn.out_proj.bias', 'model.model.decoder.layers.0.fc1.bias', 'model.model.encoder.layers.0.self_attn.q_proj.bias', 'model.model.decoder.layers.2.fc2.bias', 'model.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.model.encoder.layers.1.final_layer_norm.bias', 'model.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.model.encoder.layers.4.self_attn.v_proj.bias', 'model.model.encoder.layers.1.self_attn.v_proj.bias', 'model.model.decoder.layers.0.self_attn.v_proj.bias', 'model.model.encoder.layers.1.self_attn.out_proj.bias', 'model.model.decoder.layers.4.fc2.bias', 'model.model.encoder.layers.3.self_attn.out_proj.weight', 'model.model.decoder.layers.1.self_attn.out_proj.bias', 'model.model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.model.encoder.layers.5.self_attn_layer_norm.bias', 'model.model.decoder.layers.0.self_attn.v_proj.weight', 'model.model.decoder.layers.5.self_attn.v_proj.bias', 'model.model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.model.decoder.layers.3.fc1.bias', 'model.model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.model.encoder.layers.5.final_layer_norm.weight', 'model.model.encoder.layers.1.self_attn_layer_norm.bias', 'model.model.encoder.layers.1.self_attn.v_proj.weight', 'model.model.decoder.layers.0.self_attn_layer_norm.weight', 'model.model.encoder.layers.0.self_attn.v_proj.bias', 'model.model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.model.encoder.layers.2.self_attn.q_proj.weight', 'model.model.decoder.layers.2.self_attn.v_proj.weight', 'model.model.encoder.layers.5.self_attn.q_proj.bias', 'model.model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.model.encoder.layers.2.fc1.bias', 'model.model.decoder.layers.2.self_attn.v_proj.bias', 'model.model.encoder.layers.3.self_attn_layer_norm.weight', 'model.model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.model.decoder.layers.2.fc2.weight', 'model.model.encoder.layers.5.self_attn.k_proj.weight', 'model.model.encoder.layers.5.fc2.bias', 'model.model.decoder.layers.1.self_attn_layer_norm.bias', 'model.model.decoder.layers.4.self_attn.v_proj.weight', 'model.model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.model.decoder.layers.2.self_attn.out_proj.weight', 'model.model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.model.encoder.layers.4.fc1.weight', 'model.model.encoder.layers.2.self_attn.q_proj.bias', 'model.model.encoder.layers.3.self_attn_layer_norm.bias', 'model.model.encoder.layers.2.self_attn_layer_norm.weight', 'model.model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.model.encoder.layers.0.self_attn_layer_norm.weight', 'model.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.model.encoder.layers.4.fc2.bias', 'model.model.decoder.layers.0.self_attn.k_proj.weight', 'model.model.encoder.layers.5.self_attn.out_proj.bias', 'model.model.decoder.layers.0.self_attn.out_proj.weight', 'model.model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.model.decoder.layers.1.fc2.bias', 'model.model.encoder.layers.0.self_attn.q_proj.weight', 'model.model.encoder.layers.3.fc2.weight', 'model.model.encoder.layers.0.fc1.weight', 'model.model.decoder.layers.4.self_attn_layer_norm.weight', 'model.model.encoder.layers.4.final_layer_norm.bias', 'model.model.decoder.layers.1.fc1.bias', 'model.model.decoder.layers.0.final_layer_norm.weight', 'model.model.decoder.layers.0.self_attn.q_proj.weight', 'model.model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.model.decoder.layers.4.self_attn.k_proj.bias', 'model.model.decoder.layers.5.self_attn.v_proj.weight', 'model.model.encoder.layers.0.fc2.bias', 'model.model.encoder.layers.4.fc2.weight', 'model.model.encoder.layers.5.self_attn.out_proj.weight', 'model.model.encoder.layers.0.self_attn.k_proj.bias', 'model.model.encoder.layers.4.self_attn_layer_norm.bias', 'model.model.encoder.layers.4.final_layer_norm.weight', 'model.model.decoder.layers.3.self_attn.v_proj.weight', 'model.model.encoder.layers.0.self_attn.out_proj.weight', 'model.model.encoder.layers.4.self_attn.q_proj.bias', 'model.model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.model.decoder.layers.5.self_attn.q_proj.weight', 'model.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.model.encoder.layers.1.self_attn.out_proj.weight', 'model.model.encoder.layers.5.self_attn.v_proj.bias', 'model.model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.model.decoder.layers.3.self_attn.q_proj.bias', 'model.model.encoder.layers.3.final_layer_norm.bias', 'model.model.decoder.layers.0.fc1.weight', 'model.model.decoder.layers.5.fc1.bias', 'model.model.decoder.layers.1.fc2.weight', 'model.model.encoder.layers.1.self_attn.q_proj.weight', 'model.model.decoder.layers.5.self_attn.k_proj.bias', 'model.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.model.decoder.layers.2.self_attn.k_proj.weight', 'model.model.decoder.layers.0.self_attn.k_proj.bias', 'model.model.decoder.layers.3.self_attn.k_proj.weight', 'model.model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.model.shared.weight', 'model.model.decoder.layers.3.self_attn_layer_norm.weight', 'model.model.encoder.layers.0.fc1.bias', 'model.model.encoder.layers.1.fc2.bias', 'model.model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.model.decoder.layers.2.self_attn.q_proj.weight', 'model.model.decoder.layers.1.final_layer_norm.weight', 'model.model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.model.encoder.layers.0.final_layer_norm.bias', 'model.model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.model.decoder.layers.5.self_attn.out_proj.weight', 'model.model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.model.encoder.layers.5.self_attn.q_proj.weight', 'model.model.decoder.layers.4.self_attn.out_proj.weight', 'model.model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.model.encoder.layers.1.self_attn_layer_norm.weight', 'model.model.encoder.layers.1.fc1.weight', 'model.model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.model.encoder.layers.2.self_attn.k_proj.bias', 'model.model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.model.encoder.layers.4.self_attn.q_proj.weight', 'model.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.model.encoder.layers.0.final_layer_norm.weight', 'model.model.decoder.layers.3.final_layer_norm.bias', 'model.model.decoder.layers.4.self_attn.q_proj.weight', 'model.model.encoder.layers.3.self_attn.out_proj.bias', 'model.model.encoder.layers.0.self_attn_layer_norm.bias', 'model.model.encoder.layers.3.self_attn.q_proj.weight', 'model.model.decoder.layers.2.self_attn.k_proj.bias', 'model.model.encoder.layers.1.fc2.weight', 'model.model.encoder.layers.3.self_attn.v_proj.weight', 'model.model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.model.encoder.layers.0.self_attn.k_proj.weight', 'model.model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.model.encoder.layers.3.fc1.weight', 'model.model.decoder.layers.2.final_layer_norm.bias', 'model.model.decoder.layers.2.final_layer_norm.weight', 'model.model.encoder.layers.5.final_layer_norm.bias', 'model.model.decoder.layers.5.self_attn_layer_norm.weight', 'model.model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.model.decoder.layers.4.fc2.weight', 'model.model.decoder.layers.3.self_attn_layer_norm.bias', 'model.model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.model.encoder.layers.0.self_attn.v_proj.weight', 'model.model.encoder.layers.1.fc1.bias', 'model.model.encoder.layers.2.self_attn_layer_norm.bias', 'model.model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.model.encoder.layers.4.self_attn.out_proj.bias', 'model.model.decoder.layers.5.fc2.weight', 'model.model.encoder.layers.3.final_layer_norm.weight', 'model.model.encoder.layers.4.self_attn.k_proj.bias', 'model.model.decoder.layers.2.self_attn_layer_norm.weight', 'model.model.encoder.layers.2.self_attn.out_proj.bias', 'model.model.encoder.layers.4.self_attn.v_proj.weight', 'model.model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.model.decoder.layers.2.self_attn_layer_norm.bias', 'model.model.decoder.layers.3.self_attn.out_proj.weight', 'model.model.decoder.layers.4.self_attn.k_proj.weight', 'model.model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.model.decoder.layers.4.final_layer_norm.bias', 'model.model.decoder.layers.5.final_layer_norm.bias', 'model.model.decoder.layers.0.self_attn.q_proj.bias', 'model.model.decoder.layers.2.encoder_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianForMT(\n",
      "  (model): MarianMTModel(\n",
      "    (model): MarianModel(\n",
      "      (shared): Embedding(65001, 512, padding_idx=65000)\n",
      "      (encoder): MarianEncoder(\n",
      "        (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "        (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x MarianEncoderLayer(\n",
      "            (self_attn): MarianAttention(\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (activation_fn): SiLUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (decoder): MarianDecoder(\n",
      "        (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "        (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x MarianDecoderLayer(\n",
      "            (self_attn): MarianAttention(\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (activation_fn): SiLUActivation()\n",
      "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): MarianAttention(\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from modeling import MarianForMT # 为什么这里直接import另外一个文件中的类会报错？显示缺失参数？\n",
    "from transformers import AutoConfig\n",
    "from torch import nn\n",
    "from transformers.models.marian import MarianPreTrainedModel, MarianModel, MarianMTModel\n",
    "import torch\n",
    "\n",
    "# class MarianForMT(MarianMTModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.model = MarianMTModel(config=config)\n",
    "#         target_vocab_size = config.decoder_vocab_size\n",
    "#         # 这行代码的意思是在PyTorch模型中注册一个名为\"final_logits_bias\"的缓冲区，并初始化为一个全零的张量，维度为(1, target_vocab_size)。\n",
    "#         # 这个缓冲区可以被模型访问和使用，通常用于存储模型的参数或其他需要持久化的数据。\n",
    "#         self.register_buffer(\"final_logits_bias\", torch.zeros((1, target_vocab_size)))\n",
    "#         self.lm_head = nn.Linear(config.d_model, target_vocab_size, bias=False)\n",
    "#         self.post_init() # 这个方法在对象初始化完成后自动调用，可以用来执行一些需要在对象创建后立即执行的操作。\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         output = self.model(**x)\n",
    "#         sequence_output = output.last_hidden_state\n",
    "#         lm_logits = self.lm_head(sequence_output) + self.final_logits_bias\n",
    "#         return lm_logits\n",
    "    \n",
    "#     def other_func(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model = MarianForMT.from_pretrained(model_checkpoint, config=config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.750469682990165\n",
      "1.683602693167689\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "bad_predictions_1 = [\"This This This This\"]\n",
    "bad_predictions_2 = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "bleu = BLEU()\n",
    "print(bleu.corpus_score(predictions, references).score)\n",
    "print(bleu.corpus_score(bad_predictions_1, references).score)\n",
    "print(bleu.corpus_score(bad_predictions_2, references).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 2.1476912089159055\n",
      "wrong BLEU: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "predictions = [\n",
    "    \"我在复旦大学学习摆烂，复旦大学很sb。\"\n",
    "]\n",
    "\n",
    "# references = [\n",
    "#     [\n",
    "#         \"我在环境优美的复旦大学学习躺平。\"\n",
    "#     ]\n",
    "# ]\n",
    "references = [\n",
    "    \"我在环境优美的复旦大学学习躺平。\"\n",
    "]\n",
    "\n",
    "bleu = BLEU(tokenize='zh')\n",
    "print(f'BLEU: {bleu.corpus_score(predictions, references).score}')\n",
    "bleu = BLEU()\n",
    "print(f'wrong BLEU: {bleu.corpus_score(predictions, references).score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I studied flattened at the University of Rehabilitation in a beautiful environment.', 'I was born and died at the University of Rehabilitation in a beautiful environment.']\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "orig_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "orig_model = orig_model.to(device)\n",
    "\n",
    "sentence = [\"我在环境优美的复旦大学学习躺平。\", \"我在环境优美的复旦大学卷生卷死。\"]\n",
    "sentence_inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True).to(device)\n",
    "sentence_generated_tokens = orig_model.generate(\n",
    "    sentence_inputs[\"input_ids\"],\n",
    "    attention_mask=sentence_inputs[\"attention_mask\"],\n",
    "    max_length=128\n",
    ")\n",
    "# sentence_decoded_pred = tokenizer.decode(sentence_generated_tokens[1], skip_special_tokens=True)\n",
    "sentence_decoded_pred = tokenizer.batch_decode(sentence_generated_tokens, skip_special_tokens=True)\n",
    "print(sentence_decoded_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cc987a8c0b4cc1968642757e7ec128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb 单元格 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_num):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepoch_num\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     total_loss \u001b[39m=\u001b[39m train_loop(train_dataloader, model, optimizer, lr_scheduler, t\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, total_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     valid_bleu \u001b[39m=\u001b[39m test_loop(tokenizer, valid_dataloader, model, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m valid_bleu \u001b[39m>\u001b[39m best_bleu:\n",
      "File \u001b[0;32m~/code/project/NLP-proj/seq2seq/run_sim_cls.py:18\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, optimizer, lr_scheduler, epoch, total_loss)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m batch, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     17\u001b[0m     batch_data \u001b[39m=\u001b[39m batch_data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m     outputs \u001b[39m=\u001b[39m model(batch_data)\n\u001b[1;32m     19\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     20\u001b[0m     \u001b[39m# 实际上的计算方法\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39m# loss_fct = CrossEntropyLoss()\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# loss = loss_fct(lm_logits.view(-1, self.config.decoder_vocab_size), labels.view(-1))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/project/NLP-proj/seq2seq/modeling.py:25\u001b[0m, in \u001b[0;36mMarianForMT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mx)\n\u001b[1;32m     24\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mdecoder_hidden_states\n\u001b[0;32m---> 25\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(sequence_output) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m     27\u001b[0m return_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     28\u001b[0m labels \u001b[39m=\u001b[39m x[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "from run_sim_cls import train_loop, test_loop\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 3\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=epoch_num * len(train_dataloader),\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "total_loss = 0\n",
    "best_bleu = 0\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_bleu = test_loop(tokenizer, valid_dataloader, model, mode='Valid')\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'../model/seq2seq/epoch_{t+1}_valid_bleu_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "test_data = TRANS('../../data/translation2019zh/translation2019zh_valid.json')\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load('../model/seq2seq/epoch_1_valid_bleu_model_weights.bin'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    sources, preds, labels = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "        ).cpu().numpy()\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_sources = tokenizer.batch_decode(\n",
    "            batch_data[\"input_ids\"].cpu().numpy(), \n",
    "            skip_special_tokens=True, \n",
    "            use_source_tokenizer=True\n",
    "        )\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sources += [source.strip() for source in decoded_sources]\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    bleu_score = bleu.corpus_score(preds, labels).score\n",
    "    print(f\"Test BLEU: {bleu_score:>0.2f}\\n\")\n",
    "    results = []\n",
    "    print('saving predicted results...')\n",
    "    for source, pred, label in zip(sources, preds, labels):\n",
    "        results.append({\n",
    "            \"sentence\": source, \n",
    "            \"prediction\": pred, \n",
    "            \"translation\": label[0]\n",
    "        })\n",
    "    with open('../model/seq2seq/test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for exapmle_result in results:\n",
    "            f.write(json.dumps(exapmle_result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
