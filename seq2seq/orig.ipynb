{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 200000\n",
      "valid set size: 20001\n",
      "test set size: 39323\n",
      "{'english': 'Zock said it\\'s too soon to tell people to swear off spray cleaners altogether, but added, \"Nevertheless, from the perspective of precaution, we may recommend to use sprays only when really necessary.', 'chinese': 'Zock说，现在就告诫人们远离喷雾清洁剂还为时尚早，但他补充说：“毫无疑问，从预防的角度来看，我们推荐仅仅在必要的时候才使用喷雾剂。'}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    "from data import TRANS\n",
    "\n",
    "max_dataset_size = 220000\n",
    "train_dataset_size = 200000\n",
    "valid_dataset_size = 20001\n",
    "\n",
    "data = TRANS('../../data/translation2019zh/translation2019zh_train.json')\n",
    "train_data, valid_data = random_split(data, [train_dataset_size, valid_dataset_size])\n",
    "test_data = TRANS('../../data/translation2019zh/translation2019zh_valid.json')    \n",
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Z', 'ock', '说', ',', '现在就', '告诫', '人们', '远离', '喷', '雾', '清洁', '剂', '还为', '时', '尚', '早', ',', '但他', '补充说', ':“', '毫无疑问', ',', '从', '预防', '的', '角度来看', ',', '我们', '推荐', '仅仅', '在', '必要', '的时候', '才', '使用', '喷', '雾', '剂', '。', '</s>']\n",
      "['▁Z', 'ock', '▁said', '▁it', \"'\", 's', '▁too', '▁soon', '▁to', '▁tell', '▁people', '▁to', '▁swear', '▁off', '▁spray', '▁cleaner', 's', '▁altogether', ',', '▁but', '▁added', ',', '▁\"', 'Never', 'the', 'less', ',', '▁from', '▁the', '▁perspective', '▁of', '▁precaution', ',', '▁we', '▁may', '▁recommend', '▁to', '▁use', '▁spray', 's', '▁only', '▁when', '▁really', '▁necessary', '.', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "zh_sentence = train_data[0][\"chinese\"]\n",
    "en_sentence = train_data[0][\"english\"]\n",
    "\n",
    "inputs = tokenizer(zh_sentence)\n",
    "\n",
    "# # 注意添加上下文管理器\n",
    "# wrong_targets = tokenizer(en_sentence)\n",
    "# print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(en_sentence)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 40]), 'attention_mask': torch.Size([4, 40])}\n",
      "batch_y shape: torch.Size([4, 46])\n",
      "{'input_ids': tensor([[ 6932, 14872,   300,     2, 20172, 37620,  2283, 20999, 14522, 26668,\n",
      "         10696,  5898, 16604,   142,  5942,  4966,     2, 22937, 25217,  7306,\n",
      "         30508,     2,   233,  1889,    11, 22487,     2,   230, 17586,  8195,\n",
      "            36,  4634,  2408,  1880,   421, 14522, 26668,  5898,     9,     0],\n",
      "        [    7, 36783,    36,  5564,  3156,   150, 13633,  4571,  2806,     2,\n",
      "          5454,   242,    16,   391,  2936,    11, 55220, 20303,   123,  3286,\n",
      "             2,   333,  1473,  1400,    36,  1284, 26531,     9,     0, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000],\n",
      "        [ 1243,  1956,  7870, 23809, 16376,  5264,     2,  1132, 25995,  2505,\n",
      "          2837,  8509,  1041,   485, 22857, 37082,     2, 33218, 27876,  3284,\n",
      "          8359, 19522,    11, 27955,     9,     0, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000],\n",
      "        [  799,   142,     2, 12944,  1073, 48343, 21359,   375,  3212, 30248,\n",
      "         15417,     9,  1075, 10103, 29274,     2,  3165, 29456,  1773, 20243,\n",
      "             2,  1773, 34805,     2, 19346, 38785,     2,  3585,  7231,   408,\n",
      "          1072,  3078,  4698,     9,     0, 65000, 65000, 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n",
      "tensor([[ 6932, 14872,   412,    39,    21,    22,   987,  2354,     8,  1266,\n",
      "           235,     8, 14308,  1122, 46805, 23396,    22, 38707,     2,   183,\n",
      "          4578,     2,    38, 60804,  1843,  7111,     2,    49,     3,  3749,\n",
      "             4, 56699,     2,   107,   252,  6575,     8,   283, 46805,    22,\n",
      "           273,   346,  1214,   525,     5,     0],\n",
      "        [  206,  4827,  1836,  1298,  1430,  7741,    18, 48358, 10622,     2,\n",
      "            26,   559,     3, 44780,  5375,  5769,    19,    79, 12650,    27,\n",
      "            56,  1775,  1033,     2,  1217,   792,   245,    32,    10,  2394,\n",
      "         12139,   250,     7,     5,     0,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   82,    21,    22,    79,    12,  1033,     8,   721,   199,  3230,\n",
      "            18,    61, 22488,    22,     4,  1494,   343,     6,  1450,     3,\n",
      "          5906,     8,  1277,   742, 23225,     5,     0,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  403, 40440,  6776,     2,     3,  2666,    73,  1415, 34314,     6,\n",
      "           235,   252,  2708, 48940,     2,  1517, 43548, 40941,     2, 31542,\n",
      "         24546,     2,    50,  2818,  4368,     4, 24174,    33,    12,   979,\n",
      "             4, 48922, 19862,    86,     5,     0,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "inputs = [train_data[s_idx][\"chinese\"] for s_idx in range(4)]\n",
    "targets = [train_data[s_idx][\"english\"] for s_idx in range(4)]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    inputs, \n",
    "    padding=True, \n",
    "    max_length=max_input_length, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        padding=True, \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "for idx, end_idx in enumerate(end_token_index):\n",
    "    labels[idx][end_idx+1:] = -100 # 是否可以认为，pad设置为-100是为了让softmax之后的权重置为0？\n",
    "\n",
    "print('batch_X shape:', {k: v.shape for k, v in model_inputs.items()})\n",
    "print('batch_y shape:', labels.shape)\n",
    "print(model_inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['chinese'])\n",
    "        batch_targets.append(sample['english'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets,\n",
    "            padding=True,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"]\n",
    "        batch_data[\"decode_input_ids\"] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx + 1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=32, shuffle=False, collate_fn=collote_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decode_input_ids', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([32, 41]), 'attention_mask': torch.Size([32, 41]), 'decode_input_ids': torch.Size([32, 44]), 'labels': torch.Size([32, 44])}\n",
      "{'input_ids': tensor([[ 3636, 32728,  2582,  ..., 65000, 65000, 65000],\n",
      "        [    7, 13882, 36601,  ..., 65000, 65000, 65000],\n",
      "        [  799,  5657, 17030,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [    7,  6713,  6281,  ..., 65000, 65000, 65000],\n",
      "        [13263, 39057,  8116,  ..., 65000, 65000, 65000],\n",
      "        [    7, 38443,     2,  ..., 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decode_input_ids': tensor([[65000,  2919,  2431,  ..., 65000, 65000, 65000],\n",
      "        [65000,    24, 33014,  ..., 65000, 65000, 65000],\n",
      "        [65000,  4981,    10,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [65000,  3035,    21,  ...,    21,   445,     5],\n",
      "        [65000,  4744,     7,  ..., 65000, 65000, 65000],\n",
      "        [65000,  2148, 18764,  ..., 65000, 65000, 65000]]), 'labels': tensor([[ 2919,  2431,    57,  ...,  -100,  -100,  -100],\n",
      "        [   24, 33014,  1009,  ...,  -100,  -100,  -100],\n",
      "        [ 4981,    10,  1586,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 3035,    21,    22,  ...,   445,     5,     0],\n",
      "        [ 4744,     7, 53997,  ...,  -100,  -100,  -100],\n",
      "        [ 2148, 18764,   466,  ...,  -100,  -100,  -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练代码 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MarianForMT were not initialized from the model checkpoint at Helsinki-NLP/opus-mt-zh-en and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianForMT(\n",
      "  (model): MarianModel(\n",
      "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
      "    (encoder): MarianEncoder(\n",
      "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x MarianEncoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): SiLUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): MarianDecoder(\n",
      "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x MarianDecoderLayer(\n",
      "          (self_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): SiLUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MarianAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# from modeling import MarianForMT # 为什么这里直接import另外一个文件中的类会报错？显示缺失参数？\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from torch import nn\n",
    "from transformers.models.marian import MarianPreTrainedModel, MarianModel\n",
    "import torch\n",
    "class MarianForMT(MarianPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = MarianModel(config=config)\n",
    "        target_vocab_size = config.decoder_vocab_size\n",
    "        # 这行代码的意思是在PyTorch模型中注册一个名为\"final_logits_bias\"的缓冲区，并初始化为一个全零的张量，维度为(1, target_vocab_size)。\n",
    "        # 这个缓冲区可以被模型访问和使用，通常用于存储模型的参数或其他需要持久化的数据。\n",
    "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, target_vocab_size)))\n",
    "        self.lm_head = nn.Linear(config.d_model, target_vocab_size, bias=False)\n",
    "        self.post_init() # 这个方法在对象初始化完成后自动调用，可以用来执行一些需要在对象创建后立即执行的操作。\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(**x)\n",
    "        sequence_output = output.last_hidden_state\n",
    "        lm_logits = self.lm_head(sequence_output) + self.final_logits_bias\n",
    "        return lm_logits\n",
    "    \n",
    "    def other_func(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model = MarianForMT.from_pretrained(model_checkpoint, config=config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.750469682990165\n",
      "1.683602693167689\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "bad_predictions_1 = [\"This This This This\"]\n",
    "bad_predictions_2 = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "bleu = BLEU()\n",
    "print(bleu.corpus_score(predictions, references).score)\n",
    "print(bleu.corpus_score(bad_predictions_1, references).score)\n",
    "print(bleu.corpus_score(bad_predictions_2, references).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 2.1476912089159055\n",
      "wrong BLEU: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "predictions = [\n",
    "    \"我在复旦大学学习摆烂，复旦大学很sb。\"\n",
    "]\n",
    "\n",
    "# references = [\n",
    "#     [\n",
    "#         \"我在环境优美的复旦大学学习躺平。\"\n",
    "#     ]\n",
    "# ]\n",
    "references = [\n",
    "    \"我在环境优美的复旦大学学习躺平。\"\n",
    "]\n",
    "\n",
    "bleu = BLEU(tokenize='zh')\n",
    "print(f'BLEU: {bleu.corpus_score(predictions, references).score}')\n",
    "bleu = BLEU()\n",
    "print(f'wrong BLEU: {bleu.corpus_score(predictions, references).score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I studied flattened at the University of Rehabilitation in a beautiful environment.', 'I was born and died at the University of Rehabilitation in a beautiful environment.']\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "orig_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "orig_model = orig_model.to(device)\n",
    "\n",
    "sentence = [\"我在环境优美的复旦大学学习躺平。\", \"我在环境优美的复旦大学卷生卷死。\"]\n",
    "sentence_inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True).to(device)\n",
    "sentence_generated_tokens = orig_model.generate(\n",
    "    sentence_inputs[\"input_ids\"],\n",
    "    attention_mask=sentence_inputs[\"attention_mask\"],\n",
    "    max_length=128\n",
    ")\n",
    "# sentence_decoded_pred = tokenizer.decode(sentence_generated_tokens[1], skip_special_tokens=True)\n",
    "sentence_decoded_pred = tokenizer.batch_decode(sentence_generated_tokens, skip_special_tokens=True)\n",
    "print(sentence_decoded_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e63817ada8a4b09a34bbd043b8989de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MarianForMT' object has no attribute 'prepare_decoder_input_ids_from_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb 单元格 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_num):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepoch_num\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     total_loss \u001b[39m=\u001b[39m train_loop(train_dataloader, model, optimizer, lr_scheduler, t\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, total_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     valid_bleu \u001b[39m=\u001b[39m test_loop(tokenizer, valid_dataloader, model, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m valid_bleu \u001b[39m>\u001b[39m best_bleu:\n",
      "File \u001b[0;32m~/code/project/NLP-proj/seq2seq/run_sim_cls.py:16\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, optimizer, lr_scheduler, epoch, total_loss)\u001b[0m\n\u001b[1;32m     14\u001b[0m finish_batch_num \u001b[39m=\u001b[39m (epoch \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[1;32m     15\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m batch, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     17\u001b[0m     batch_data \u001b[39m=\u001b[39m batch_data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb 单元格 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mwith\u001b[39;00m tokenizer\u001b[39m.\u001b[39mas_target_tokenizer():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     labels \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         batch_targets,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     )[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     batch_data[\u001b[39m\"\u001b[39m\u001b[39mdecode_input_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mprepare_decoder_input_ids_from_labels(labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     end_token_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(labels \u001b[39m==\u001b[39m tokenizer\u001b[39m.\u001b[39meos_token_id)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenglibin/code/project/NLP-proj/seq2seq/orig.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx, end_idx \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(end_token_index):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MarianForMT' object has no attribute 'prepare_decoder_input_ids_from_labels'"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "from run_sim_cls import train_loop, test_loop\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 3\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=epoch_num * len(train_dataloader),\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "total_loss = 0\n",
    "best_bleu = 0\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_bleu = test_loop(tokenizer, valid_dataloader, model, mode='Valid')\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_bleu_{valid_bleu:0.2f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
