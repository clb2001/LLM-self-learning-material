{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 200000\n",
      "valid set size: 20001\n",
      "test set size: 39323\n",
      "{'english': 'Part 2 discusses some approaches to tracking down memory problems.', 'chinese': '第 2 部分将讨论一些跟踪内存问题的方法。'}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    "from data import TRANS\n",
    "\n",
    "max_dataset_size = 220000\n",
    "train_dataset_size = 200000\n",
    "valid_dataset_size = 20001\n",
    "\n",
    "data = TRANS('../../data/translation2019zh/translation2019zh_train.json')\n",
    "train_data, valid_data = random_split(data, [train_dataset_size, valid_dataset_size])\n",
    "test_data = TRANS('../../data/translation2019zh/translation2019zh_valid.json')    \n",
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72022aa7ce54bc685a84d575bae15b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52088401411454da6ca845bc1ba909f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad43d0cbf818469dbd76614882fe9e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading source.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ded2fb51b24413f9530cb695cfe91f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading target.spm:   0%|          | 0.00/807k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667a9f64a3d24479a49e0cc057c3e8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenglibin/.local/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁第', '▁2', '▁', '部分', '将', '讨论', '一些', '跟踪', '内', '存', '问题', '的方法', '。', '</s>']\n",
      "['▁Part', '▁2', '▁discusses', '▁some', '▁approaches', '▁to', '▁tracking', '▁down', '▁memory', '▁problems', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "zh_sentence = train_data[0][\"chinese\"]\n",
    "en_sentence = train_data[0][\"english\"]\n",
    "\n",
    "inputs = tokenizer(zh_sentence)\n",
    "\n",
    "# # 注意添加上下文管理器\n",
    "# wrong_targets = tokenizer(en_sentence)\n",
    "# print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(en_sentence)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 31]), 'attention_mask': torch.Size([4, 31])}\n",
      "batch_y shape: torch.Size([4, 35])\n",
      "{'input_ids': tensor([[  440,   413,     7,  1054,    96,   621,   617,  7119,   475,  6060,\n",
      "           112,  3465,     9,     0, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000],\n",
      "        [    7, 13874,   521, 10612, 62719,     2,   176, 13874,     9,     0,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000],\n",
      "        [  196,   288, 21116,   449, 18051,  1924,  5318, 16539,  1447, 31324,\n",
      "          2702,  7035, 19320,  6781,  5978, 10161,   864,     9,   318,   397,\n",
      "          2220,  1437,   316,  1336,   449,   430,  1191,  8174, 17744,     9,\n",
      "             0],\n",
      "        [11363,  2271,  1491, 17615, 39577,   272,  1744,  2795,   265, 38809,\n",
      "          2452,     2,  3297,  1497,  2880,  5711,  4094,     2,  2764,   123,\n",
      "          1512,  8530,     9,     0, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[ 5346,   413, 37510,   239,  2842,     8, 13211,   791, 13729,  1207,\n",
      "             5,     0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100],\n",
      "        [23288,    58,  1698, 29827,     6, 30297,     5,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100],\n",
      "        [   38,  2848,  6010,  5231,    19,    84,   368,   611,    84,   154,\n",
      "           187, 11919,  5089,  1322,  1999, 26417,     5,  5705,  6781, 22977,\n",
      "           177,     2,   200,    21,    22,   631,   923,    19,    92,  2248,\n",
      "          6307,  8470, 41859,     5,     0],\n",
      "        [   24,  1240, 13716,  4396, 37389,     4,  1309,    64,    72,   292,\n",
      "             3,  7204, 13268,   567,  7312, 43292,    22,     5,    24,  1526,\n",
      "          5761,    30,  1344,     6,    30, 50690,     5,     0,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "inputs = [train_data[s_idx][\"chinese\"] for s_idx in range(4)]\n",
    "targets = [train_data[s_idx][\"english\"] for s_idx in range(4)]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    inputs, \n",
    "    padding=True, \n",
    "    max_length=max_input_length, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        padding=True, \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "for idx, end_idx in enumerate(end_token_index):\n",
    "    labels[idx][end_idx+1:] = -100\n",
    "\n",
    "print('batch_X shape:', {k: v.shape for k, v in model_inputs.items()})\n",
    "print('batch_y shape:', labels.shape)\n",
    "print(model_inputs)\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
